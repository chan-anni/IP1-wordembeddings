# Independent Project 1 - Semantic Representations and Bias Through Word Embeddings

This independent project explores how word embeddings capture semantic relationships between words in natural language processing (NLP) and investigates how these embeddings also reflect underlying biases, particularly regarding gender and race. Through first understanding how word embeddings work then by examining popular word embedding models, specifically Word2Vec and ELMo, this project aims to reveal how these models learn associations between words with similar meanings and carry human biases present in the training data. 

This project is motivated by a curiosity about how machines represent human language and an interest in the societal implications of these biases.

### Goals
Understand Word Relationships: Explore how embeddings represent words in different contexts and capture semantic relationships.
Analyze Bias: Investigate how embeddings encode gender and racial biases, and examine methods for quantifying and visualizing these biases.

### Methods


### Results
**Gender Bias**: Observed some projections of stereotypical gender roles within certain word groups (e.g., “nurse” closer to female-associated terms).


Setup: 
-----------------------------------------------------
download and install: 
pip install -r requirements.txt


 Setup Python 3.9.18 environment for the ELMo model 


 Setup Python 3.11 environment for "word2vec" 
